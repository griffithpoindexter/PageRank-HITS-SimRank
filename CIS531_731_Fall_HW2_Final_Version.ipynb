{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIS531_731_Fall_HW2 - Final Version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPF1rKbj1GUYizKPttKQNHb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/griffithpoindexter/PageRank-HITS-SimRank/blob/master/CIS531_731_Fall_HW2_Final_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqxMFXXMkwjI"
      },
      "source": [
        "#Homework 2 \n",
        "\n",
        "By: Griffith Poindexter \n",
        "\n",
        "\n",
        "## General info\n",
        "The objective of the code below was to fulfill the assignment requirements listed in the Homework 2 word file\n",
        "\t\n",
        "## Technologies\n",
        "Project is created with:\n",
        "* Spark \n",
        "\t\n",
        "##Files needed \n",
        "* https://www.gutenberg.org/files/66057/66057-0.txt\n",
        "* https://www.gutenberg.org/files/66057/66057-0.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyq0jDE2ZVoM"
      },
      "source": [
        "####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "!pip install -q pyspark\n",
        "!wget -q https://www.gutenberg.org/files/66057/66057-0.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ5HeAePgWV_"
      },
      "source": [
        "#import appropriate libraries and create Spark and SQL session \n",
        "import os\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alME2U1hKkY9",
        "outputId": "db186413-2b6b-4aff-ffc3-6ff9ffa64635"
      },
      "source": [
        "# create removePunctuation function to clean data \n",
        "import re\n",
        "import string\n",
        "def removePunctuation(text):\n",
        "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
        "\n",
        "    Note:\n",
        "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
        "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
        "        punctuation is removed.\n",
        "\n",
        "    Args:\n",
        "        text (str): A string.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned up string.\n",
        "    \"\"\"\n",
        "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    return regex.sub('', text).lower().strip()\n",
        "    \n",
        "print (removePunctuation('YŪGAO'))\n",
        "print (removePunctuation(' No under_score!'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yūgao\n",
            "no underscore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btI-jEpvJOZt",
        "outputId": "834c10ff-9c14-45c9-9abc-727c77063970"
      },
      "source": [
        "# create removePunctuation function that does not lower case items, removes numbers, and removes roman numerals \n",
        "import re\n",
        "import string\n",
        "def removePunctuationupper(text):\n",
        "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
        "\n",
        "    Note:\n",
        "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
        "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
        "        punctuation is removed.\n",
        "\n",
        "    Args:\n",
        "        text (str): A string.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned up string.\n",
        "    \"\"\"\n",
        "    replace_dashes = text.replace(\"-\",\" \")\n",
        "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    data = regex.sub('', replace_dashes).strip()\n",
        "    remove_numbers = re.compile('[0-9]')\n",
        "    data_no_numbers = remove_numbers.sub(' ',data)\n",
        "    remove_numerals = re.compile(r'(?:^|\\s)(IX|IV|V?I{0,3})(?:\\s|$)')\n",
        "    return remove_numerals.sub(' ',data_no_numbers)\n",
        "\n",
        "#Notice the difference between the \n",
        "print (removePunctuationupper('YŪGAO12'))\n",
        "print (removePunctuationupper('Lady-in-Perpetual-Attendance'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YŪGAO \n",
            "Lady in Perpetual Attendance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFiIaXshKFfi"
      },
      "source": [
        "#create function to remove additional types of punctuation that may have been missed by the removePunctuation function \n",
        "import sys\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "punc_list=[]\n",
        "\n",
        "for i in range(0, sys.maxunicode + 1):\n",
        "  if unicodedata.category(chr(i)) in ('Pc', 'Pd', 'Pe', 'Pf', 'Pi', 'Po', 'Ps'):\n",
        "    punc_list.append(chr(i))\n",
        "\n",
        "punc_string=''.join(punc_list)\n",
        "\n",
        "def removeUniPunctuation(text):\n",
        "    regex = re.compile('[%s]' % re.escape(punc_string))\n",
        "    return regex.sub(' ', text).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugNbYuORgpYZ",
        "outputId": "5d380323-1522-424a-b665-01bb8e74822b"
      },
      "source": [
        "#import files and parse to 10 different mappers \n",
        "from pyspark.sql.functions import regexp_extract, col\n",
        "\n",
        "fileName = sc.textFile('/content/sample_data/66057-0.txt',10)\n",
        "\n",
        "remove_numerals = re.compile(r'(?:^|\\s)(IX|IV|V?I{0,3})(?:\\s|$)')\n",
        "\n",
        "#first attempt to remove punctuation from data \n",
        "fileName2 = sc.textFile('/content/sample_data/66057-0.txt',10).map(lambda x: removeUniPunctuation(x))\n",
        "\n",
        "#split all words into their own location using .split \n",
        "all_words_split = fileName.flatMap(lambda x: x.split())\n",
        "\n",
        "\n",
        "#search for uppercase letters using regex, map roman number data cleaning logic to strings if the length of the value is > 1, split data into individual cells and count occurrences  \n",
        "total_uppercase = fileName2.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count()\n",
        "\n",
        "#search for lowercase letters using regex, map roman number data cleaning logic to strings if the length of the value is > 1, split data into individual cells and count occurrences  \n",
        "total_lowercase = fileName2.flatMap(lambda x: re.findall(r'\\b[a-z]+(?:\\s+[a-z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count()\n",
        "\n",
        "print('total number of uppercase words: ', total_uppercase)\n",
        "print('total number of lowercase words: ', total_lowercase)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of uppercase words:  1403\n",
            "total number of lowercase words:  89356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_U345VgAYJp",
        "outputId": "5ba4453e-531f-4c8e-ec51-e8e9fdaa1129"
      },
      "source": [
        "#total count of all words in filename - 1. filename2 has logic that incorrectly expands the total word count, so it cannot be used. \n",
        "total_word_count  = all_words_split.count()\n",
        "total_word_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101277"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2gEaprGg7Qt"
      },
      "source": [
        "#read data accross 10 mappers \n",
        "#join string into one single line of text \n",
        "#split texts based on CHAPTER # and Appendix 3 \n",
        "\n",
        "import re\n",
        "\n",
        "book = sc.textFile('/content/sample_data/66057-0.txt',10).collect()\n",
        "\n",
        "book_single_string = ' '.join(book)\n",
        "\n",
        "chapters = re.split('CHAPTER \\w{1,4} | APPENDIX \\w{1}',book_single_string)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyXGpL_Y8zpR",
        "outputId": "961da07d-1e59-44f6-f323-f473a36f94c1"
      },
      "source": [
        "#parralize chapter data into their own RDD for analysis \n",
        "intro = sc.parallelize([chapters[0]])\n",
        "chapter1 = sc.parallelize([chapters[1]])\n",
        "chapter2 = sc.parallelize([chapters[2]])\n",
        "chapter3 = sc.parallelize([chapters[3]])\n",
        "chapter4 = sc.parallelize([chapters[4]])\n",
        "chapter5 = sc.parallelize([chapters[5]])\n",
        "chapter6 = sc.parallelize([chapters[6]])\n",
        "chapter7 = sc.parallelize([chapters[7]])\n",
        "chapter8 = sc.parallelize([chapters[8]])\n",
        "chapter9 = sc.parallelize([chapters[9]])\n",
        "appendix = sc.parallelize([chapters[10] + chapters[11]])\n",
        "\n",
        "#run punctuation logic and place word values into their own lists location \n",
        "intro_rdd_word_count = intro.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter1_rdd_word_count = chapter1.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter2_rdd_word_count = chapter2.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter3_rdd_word_count = chapter3.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter4_rdd_word_count = chapter4.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter5_rdd_word_count = chapter5.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter6_rdd_word_count = chapter6.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter7_rdd_word_count = chapter7.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter8_rdd_word_count = chapter8.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter9_rdd_word_count = chapter9.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "appendix_rdd_word_count = appendix.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "\n",
        "\n",
        "#percentage of uppercase values by introduction, chapter, appendix \n",
        "print('percentage of uppercase values in introduction: ', round(intro_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 1: ',round(chapter1_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 2: ',round(chapter2_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 3: ',round(chapter3_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 4: ',round(chapter4_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 5: ',round(chapter5_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 6: ',round(chapter6_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 7: ',round(chapter7_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 8: ',round(chapter8_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 9: ',round(chapter9_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in appendix: ',round(appendix_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "percentage of uppercase values in introduction:  6.27\n",
            "percentage of uppercase values in chapter 1:  4.35\n",
            "percentage of uppercase values in chapter 2:  18.96\n",
            "percentage of uppercase values in chapter 3:  2.71\n",
            "percentage of uppercase values in chapter 4:  13.11\n",
            "percentage of uppercase values in chapter 5:  16.18\n",
            "percentage of uppercase values in chapter 6:  7.7\n",
            "percentage of uppercase values in chapter 7:  4.21\n",
            "percentage of uppercase values in chapter 8:  2.14\n",
            "percentage of uppercase values in chapter 9:  8.55\n",
            "percentage of uppercase values in appendix:  16.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRpQVXr9ir5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99a2b7b0-efb4-4c81-fae7-625fea6daf09"
      },
      "source": [
        "#import schema related functions \n",
        "from pyspark.sql.types import StructType\n",
        "from pyspark.sql.types import StructField\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import lower, col\n",
        "\n",
        "#look for these words in the dataframe \n",
        "word_search = ['Aoi','Emperor','Genji','Kiritsubo', 'Kōkiden','Koremitsu']\n",
        "\n",
        "#create datafrme of wordsearch values \n",
        "df_word_search = sqlContext.createDataFrame(word_search, StringType())\n",
        "\n",
        "#create data frame with remove punctuation - upper function \n",
        "dfintro = sqlContext.createDataFrame(intro_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df1 = sqlContext.createDataFrame(chapter1_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df2 = sqlContext.createDataFrame(chapter2_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df3 = sqlContext.createDataFrame(chapter3_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df4 = sqlContext.createDataFrame(chapter4_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df5 = sqlContext.createDataFrame(chapter5_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df6 = sqlContext.createDataFrame(chapter6_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df7 = sqlContext.createDataFrame(chapter7_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df8 = sqlContext.createDataFrame(chapter8_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df9 = sqlContext.createDataFrame(chapter9_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "dfappendix = sqlContext.createDataFrame(appendix_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "\n",
        "#count the occurends of the word search values by chapter \n",
        "print('introduction count: ',dfintro[dfintro['value'].isin(word_search)].count())\n",
        "print('chapter 1 count: ', df1[df1['value'].isin(word_search)].count())\n",
        "print('chapter 2 count: ',df2[df2['value'].isin(word_search)].count())\n",
        "print('chapter 3 count: ',df3[df3['value'].isin(word_search)].count())\n",
        "print('chapter 4 count: ',df4[df4['value'].isin(word_search)].count())\n",
        "print('chapter 5 count: ',df5[df5['value'].isin(word_search)].count())\n",
        "print('chapter 6 count: ',df6[df6['value'].isin(word_search)].count())\n",
        "print('chapter 7 count: ',df7[df7['value'].isin(word_search)].count())\n",
        "print('chapter 8 count: ',df8[df8['value'].isin(word_search)].count())\n",
        "print('chapter 9 count: ',df9[df9['value'].isin(word_search)].count())\n",
        "print('appendix count: ',dfappendix[dfappendix['value'].isin(word_search)].count())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "introduction count:  46\n",
            "chapter 1 count:  83\n",
            "chapter 2 count:  93\n",
            "chapter 3 count:  30\n",
            "chapter 4 count:  155\n",
            "chapter 5 count:  156\n",
            "chapter 6 count:  85\n",
            "chapter 7 count:  131\n",
            "chapter 8 count:  50\n",
            "chapter 9 count:  189\n",
            "appendix count:  14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdLtWtI7iiF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1827d2-e77a-4cac-95bc-af1af1a155f9"
      },
      "source": [
        "#read in file name and run removePunctuation - upper to clean data \n",
        "filename_with_data_cleaned_when_create = fileName.map(removePunctuationupper)\n",
        "\n",
        "#split word values into seperate list locations \n",
        "file_name_prep_for_unique_count = filename_with_data_cleaned_when_create.flatMap(lambda x: x.split())\n",
        "\n",
        "#count words, reduce by key values and count them to obtain unique word values \n",
        "unique_count_of_words = file_name_prep_for_unique_count.map(lambda word: (word,1)).reduceByKey(lambda a,b: a+b).count()\n",
        "\n",
        "print('number of unique words: ', unique_count_of_words)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of unique words:  9379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjLb2TErkM6l"
      },
      "source": [
        "Homework-2\n",
        "Question-3: Fill out the right hand side and/or below #TODO comments\n",
        "Setup the system and environment variables\n",
        "\n",
        "You need to work on Project Gutenberg edition of Part I (the first 9 chapters) of Arthur Waley’s translation of Murasaki Shikibu’s The Tale of Genji (https://www.gutenberg.org/files/66057/66057-0.txt)\n",
        "\n",
        "Sort the unique words in the book based on both words and counts in assending and descending order.\n",
        "\n",
        "Split the 61 chapters (dataframe) into 6 bins of 10 chapters in each: Ch-1 to Ch-10, Ch-11 to Ch-20, Ch-21 to Ch-30, Ch-31 to Ch-40, Ch-41 to Ch-50, Ch-51 to End. The code to split the book in to a dataframe with chapter-number and text column is given.\n",
        "\n",
        "Randomly shuffle the dataframe containing 61 chapters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J6LCvfwkP9O",
        "outputId": "fa1cd94b-dc28-4f3f-a348-e9408499df5a"
      },
      "source": [
        "####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "!pip install -q pyspark\n",
        "!wget -q https://www.gutenberg.org/files/1342/1342-0.txt\n",
        "\n",
        "####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "import os\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "import os.path\n",
        "baseDir = os.path.join('data')\n",
        "inputPath = os.path.join('/content/1342-0.txt')\n",
        "fileName = os.path.join(baseDir, inputPath)\n",
        "print(fileName)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/1342-0.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMCxozTzkYIt"
      },
      "source": [
        "Drop files to upload them to session storage\n",
        "Disk\n",
        "70.19 GB available\n",
        "Homework-2\n",
        "Question-3: Fill out the right hand side and/or below #TODO comments\n",
        "Setup the system and environment variables\n",
        "\n",
        "You need to work on Project Gutenberg edition of Part I (the first 9 chapters) of Arthur Waley’s translation of Murasaki Shikibu’s The Tale of Genji (https://www.gutenberg.org/files/66057/66057-0.txt)\n",
        "\n",
        "Sort the unique words in the book based on both words and counts in assending and descending order.\n",
        "\n",
        "Split the 61 chapters (dataframe) into 6 bins of 10 chapters in each: Ch-1 to Ch-10, Ch-11 to Ch-20, Ch-21 to Ch-30, Ch-31 to Ch-40, Ch-41 to Ch-50, Ch-51 to End. The code to split the book in to a dataframe with chapter-number and text column is given.\n",
        "\n",
        "Randomly shuffle the dataframe containing 61 chapters.\n",
        "\n",
        "[7]\n",
        "####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "!pip install -q pyspark\n",
        "!wget -q https://www.gutenberg.org/files/1342/1342-0.txt\n",
        "\n",
        "####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "import os\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "import os.path\n",
        "baseDir = os.path.join('data')\n",
        "inputPath = os.path.join('/content/1342-0.txt')\n",
        "fileName = os.path.join(baseDir, inputPath)\n",
        "print(fileName)\n",
        "[8]\n",
        "[9]\n",
        "/content/1342-0.txt\n",
        "Sorting: orderBy() function is described at https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
        "Load the text file using SparkContext (Provided below)\n",
        "get the word counts RDD\n",
        "Convert word counts RDD to dataframe and print 5 entries\n",
        "Sort dataframe by words and counts in assending order and print 5 entries for both\n",
        "Sort dataframe by words and counts in descending order and print 5 entries for both\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW6VJz9JkQPf",
        "outputId": "a78fd9db-fd5f-4f0a-c8b5-ea6dd1952c19"
      },
      "source": [
        "####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "# Remove Unicode Punctuation\n",
        "\n",
        "import sys\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "punc_list=[]\n",
        "\n",
        "for i in range(0, sys.maxunicode + 1):\n",
        "  if unicodedata.category(chr(i)) in ('Pc', 'Pd', 'Pe', 'Pf', 'Pi', 'Po', 'Ps'):\n",
        "    punc_list.append(chr(i))\n",
        "\n",
        "punc_string=''.join(punc_list)\n",
        "\n",
        "def removeUniPunctuation(text):\n",
        "    regex = re.compile('[%s]' % re.escape(punc_string))\n",
        "    return regex.sub('', text).strip()\n",
        "# Test\n",
        "print (removeUniPunctuation('s wise: “Merlin”, so far as my'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s wise Merlin so far as my\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45GtwvQskgXK",
        "outputId": "798de900-f71f-4071-cedc-cb06d60885ed"
      },
      "source": [
        "from pyspark.sql.functions import col ####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "from pyspark.sql import Row ####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "from pyspark.sql.types import * ####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "from pyspark.sql.functions import desc \n",
        "\n",
        "# TODO: specify the path to the input file (String)\n",
        "ipFilePath = '/content/sample_data/1342-0.txt'\n",
        "words = sc.textFile(ipFilePath).flatMap(lambda line: line.split(\" \"))\n",
        "print (type(words))\n",
        "#print words.take(20)\n",
        "\n",
        "# get word-count RDD\n",
        "# TODO\n",
        "wordCounts = words.map(removeUniPunctuation).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
        "#print wordCounts.take(20)\n",
        "\n",
        "# Convert word-count RDD to dataframe with 2 columns [\"word\",\"count\"]\n",
        "# TODO\n",
        "wcDF = sqlContext.createDataFrame(wordCounts, [\"word\",\"count\"])\n",
        "print (type(wcDF))\n",
        "wcDF.show(5)\n",
        "\n",
        "# sorting in assending order\n",
        "print ('sorted by word: assending')\n",
        "# TODO\n",
        "wcDF.orderBy([\"word\", \"count\"], ascending=[0, 1]).show()\n",
        "\n",
        "print ('sorted by count: assending')\n",
        "# TODO\n",
        "wcDF.orderBy([\"count\", \"word\"], ascending=[0, 1]).show()\n",
        "\n",
        "# sorting in descending order\n",
        "print ('sorted by word: descending')\n",
        "# TODO\n",
        "wcDF.orderBy(desc(\"word\"), \"count\").show(10)\n",
        "\n",
        "print ('sorted by count: descending')\n",
        "# TODO\n",
        "wcDF.orderBy(desc(\"count\"), \"word\").show(10)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.PipelinedRDD'>\n",
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "+-------+-----+\n",
            "|   word|count|\n",
            "+-------+-----+\n",
            "|    The|  285|\n",
            "|Project|   84|\n",
            "|     of| 3698|\n",
            "|  Pride|    5|\n",
            "|   Jane|  262|\n",
            "+-------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "sorted by word: assending\n",
            "+-----------------+-----+\n",
            "|             word|count|\n",
            "+-----------------+-----+\n",
            "|            éclat|    1|\n",
            "|           youths|    1|\n",
            "|            youth|    9|\n",
            "|       yourselves|    2|\n",
            "|      yourselfand|    1|\n",
            "|         yourself|   49|\n",
            "|            yours|   14|\n",
            "|             your|  419|\n",
            "|         youngest|   13|\n",
            "|          younger|   29|\n",
            "|            young|  127|\n",
            "|            youll|    1|\n",
            "|           youhow|    1|\n",
            "|           youhad|    1|\n",
            "|           youbut|    2|\n",
            "|            yoube|    1|\n",
            "|           youand|    2|\n",
            "|            youMr|    1|\n",
            "|              you| 1196|\n",
            "|yieldingcertainly|    1|\n",
            "+-----------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "sorted by count: assending\n",
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "|    |72902|\n",
            "| the| 4220|\n",
            "|  to| 4154|\n",
            "|  of| 3698|\n",
            "| and| 3445|\n",
            "| her| 2129|\n",
            "|   I| 2051|\n",
            "|   a| 1946|\n",
            "| was| 1843|\n",
            "|  in| 1834|\n",
            "|that| 1528|\n",
            "| not| 1422|\n",
            "| she| 1380|\n",
            "|  it| 1287|\n",
            "|  be| 1249|\n",
            "| you| 1196|\n",
            "| his| 1188|\n",
            "| had| 1150|\n",
            "|  as| 1135|\n",
            "|  he| 1098|\n",
            "+----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "sorted by word: descending\n",
            "+-----------+-----+\n",
            "|       word|count|\n",
            "+-----------+-----+\n",
            "|      éclat|    1|\n",
            "|     youths|    1|\n",
            "|      youth|    9|\n",
            "| yourselves|    2|\n",
            "|yourselfand|    1|\n",
            "|   yourself|   49|\n",
            "|      yours|   14|\n",
            "|       your|  419|\n",
            "|   youngest|   13|\n",
            "|    younger|   29|\n",
            "+-----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "sorted by count: descending\n",
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "|    |72902|\n",
            "| the| 4220|\n",
            "|  to| 4154|\n",
            "|  of| 3698|\n",
            "| and| 3445|\n",
            "| her| 2129|\n",
            "|   I| 2051|\n",
            "|   a| 1946|\n",
            "| was| 1843|\n",
            "|  in| 1834|\n",
            "+----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E58-zs3ukiAO",
        "outputId": "0fdeda1f-fc43-4260-f386-97b43ea39cf2"
      },
      "source": [
        "import unicodedata\n",
        "import re\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "################## Code for getting chapters out of book #####################\n",
        "\n",
        "def normalize_text(text):\n",
        "    #if not isinstance(text, str): ####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "    #text = unicode(text, 'utf-8')\n",
        "    #text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore') \n",
        "    return text\n",
        "\n",
        "def remove_white_spaces(text):\n",
        "    text = re.sub('\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "with open('/content/sample_data/1342-0.txt', 'r') as content_file:\n",
        "    content_read = content_file.read()\n",
        "\n",
        "\n",
        "content = remove_white_spaces(normalize_text(content_read))\n",
        "\n",
        "start_str = 'Chapter 60 Chapter 61' ####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "end_str = 'End of the Project Gutenberg EBook of Pride and Prejudice, by Jane Austen'\n",
        "actual_content = content[content.find(start_str)+len(start_str):content.rfind(end_str)]\n",
        "\n",
        "\n",
        "\n",
        "chapters = re.split(\" Chapter [0-9]+ \", actual_content, flags=re.IGNORECASE)[1:]\n",
        "\n",
        "ch_data = []\n",
        "for i in range(1,1+len(chapters)):\n",
        "    ch_data.append((i,chapters[i-1]))\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "# Get the dataframe out of list of tuples with columns as [\"chapter\", \"text\"]\n",
        "# TODO\n",
        "\n",
        "df = sqlContext.createDataFrame(ch_data, [\"chapter\", \"text\"])\n",
        "\n",
        "#chapter_list = df.withColumn(\"chapter\",col(\"chapter\").cast('float')).collect()\n",
        "\n",
        "df2 = df.withColumn(\"chapter\",col(\"chapter\").cast('float'))\n",
        "\n",
        "#chapter_list = [chapter_list[x][0] for x in range(len(chapter_list))]\n",
        "\n",
        "# do the binning using Bucketizer\n",
        "# TODO\n",
        "\n",
        "bucketizer = Bucketizer(splits= [0,21,41,float('Inf')], inputCol= \"chapter\", outputCol = 'buckets')\n",
        "\n",
        "df_buck = bucketizer.setHandleInvalid(\"keep\").transform(df2) \n",
        "\n",
        "df_buck.show(61)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+-------+\n",
            "|chapter|                text|buckets|\n",
            "+-------+--------------------+-------+\n",
            "|    1.0|It is a truth uni...|    0.0|\n",
            "|    2.0|Mr. Bennet was am...|    0.0|\n",
            "|    3.0|Not all that Mrs....|    0.0|\n",
            "|    4.0|When Jane and Eli...|    0.0|\n",
            "|    5.0|Within a short wa...|    0.0|\n",
            "|    6.0|The ladies of Lon...|    0.0|\n",
            "|    7.0|Mr. Bennet’s prop...|    0.0|\n",
            "|    8.0|At five o’clock t...|    0.0|\n",
            "|    9.0|Elizabeth passed ...|    0.0|\n",
            "|   10.0|The day passed mu...|    0.0|\n",
            "|   11.0|When the ladies r...|    0.0|\n",
            "|   12.0|In consequence of...|    0.0|\n",
            "|   13.0|“I hope, my dear,...|    0.0|\n",
            "|   14.0|During dinner, Mr...|    0.0|\n",
            "|   15.0|Mr. Collins was n...|    0.0|\n",
            "|   16.0|As no objection w...|    0.0|\n",
            "|   17.0|Elizabeth related...|    0.0|\n",
            "|   18.0|Till Elizabeth en...|    0.0|\n",
            "|   19.0|The next day open...|    0.0|\n",
            "|   20.0|Mr. Collins was n...|    0.0|\n",
            "|   21.0|The discussion of...|    1.0|\n",
            "|   22.0|The Bennets were ...|    1.0|\n",
            "|   23.0|Elizabeth was sit...|    1.0|\n",
            "|   24.0|Miss Bingley’s le...|    1.0|\n",
            "|   25.0|After a week spen...|    1.0|\n",
            "|   26.0|Mrs. Gardiner’s c...|    1.0|\n",
            "|   27.0|With no greater e...|    1.0|\n",
            "|   28.0|Every object in t...|    1.0|\n",
            "|   29.0|Mr. Collins’s tri...|    1.0|\n",
            "|   30.0|Sir William staye...|    1.0|\n",
            "|   31.0|Colonel Fitzwilli...|    1.0|\n",
            "|   32.0|Elizabeth was sit...|    1.0|\n",
            "|   33.0|More than once di...|    1.0|\n",
            "|   34.0|When they were go...|    1.0|\n",
            "|   35.0|Elizabeth awoke t...|    1.0|\n",
            "|   36.0|If Elizabeth, whe...|    1.0|\n",
            "|   37.0|The two gentlemen...|    1.0|\n",
            "|   38.0|On Saturday morni...|    1.0|\n",
            "|   39.0|It was the second...|    1.0|\n",
            "|   40.0|Elizabeth’s impat...|    1.0|\n",
            "|   41.0|The first week of...|    2.0|\n",
            "|   42.0|Had Elizabeth’s o...|    2.0|\n",
            "|   43.0|Elizabeth, as the...|    2.0|\n",
            "|   44.0|Elizabeth had set...|    2.0|\n",
            "|   45.0|Convinced as Eliz...|    2.0|\n",
            "|   46.0|Elizabeth had bee...|    2.0|\n",
            "|   47.0|“I have been thin...|    2.0|\n",
            "|   48.0|The whole party w...|    2.0|\n",
            "|   49.0|Two days after Mr...|    2.0|\n",
            "|   50.0|Mr. Bennet had ve...|    2.0|\n",
            "|   51.0|Their sister’s we...|    2.0|\n",
            "|   52.0|Elizabeth had the...|    2.0|\n",
            "|   53.0|Mr. Wickham was s...|    2.0|\n",
            "|   54.0|As soon as they w...|    2.0|\n",
            "|   55.0|A few days after ...|    2.0|\n",
            "|   56.0|One morning, abou...|    2.0|\n",
            "|   57.0|The discomposure ...|    2.0|\n",
            "|   58.0|Instead of receiv...|    2.0|\n",
            "|   59.0|“My dear Lizzy, w...|    2.0|\n",
            "|   60.0|Elizabeth’s spiri...|    2.0|\n",
            "|   61.0|Happy for all her...|    2.0|\n",
            "+-------+--------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYFM8CsUkpUw"
      },
      "source": [
        "Shuffling: https://stackoverflow.com/questions/43637625/how-to-shuffle-the-rows-in-a-spark-dataframe\n",
        "Print 5 entries of the dataframe containing all 61 chapters\n",
        "Use pyspark.sql.functions.rand function for randomly shuffling the dataframe containing all 61 chapters and print 5 entries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBb_PDCLkl2F",
        "outputId": "7db31be7-91d9-4d45-e0fb-68b5a01277cc"
      },
      "source": [
        "from pyspark.sql.functions import rand \n",
        "# print 5 entries of the dataframe containing all 61 chapters\n",
        "# TODO\n",
        "df_buck.show(5)\n",
        "\n",
        "\n",
        "# randomly shuffle the dataframe and print 5 entries\n",
        "# TODO\n",
        "\n",
        "df_buck.orderBy(rand()).show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+-------+\n",
            "|chapter|                text|buckets|\n",
            "+-------+--------------------+-------+\n",
            "|    1.0|It is a truth uni...|    0.0|\n",
            "|    2.0|Mr. Bennet was am...|    0.0|\n",
            "|    3.0|Not all that Mrs....|    0.0|\n",
            "|    4.0|When Jane and Eli...|    0.0|\n",
            "|    5.0|Within a short wa...|    0.0|\n",
            "+-------+--------------------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+--------------------+-------+\n",
            "|chapter|                text|buckets|\n",
            "+-------+--------------------+-------+\n",
            "|   40.0|Elizabeth’s impat...|    1.0|\n",
            "|   30.0|Sir William staye...|    1.0|\n",
            "|   26.0|Mrs. Gardiner’s c...|    1.0|\n",
            "|   61.0|Happy for all her...|    2.0|\n",
            "|   19.0|The next day open...|    0.0|\n",
            "+-------+--------------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}