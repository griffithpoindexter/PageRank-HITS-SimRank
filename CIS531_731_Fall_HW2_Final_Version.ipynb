{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIS531_731_Fall_HW2 - Final Version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNrPB2MbCPNkPiMooIT2IYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/griffithpoindexter/PageRank-HITS-SimRank/blob/master/CIS531_731_Fall_HW2_Final_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyq0jDE2ZVoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9149e74-4383-40d7-9dfe-8f578f22b0e7"
      },
      "source": [
        "####### CHANGES MADE BY INSTRUCTOR\" ########\n",
        "!pip install -q pyspark\n",
        "!wget -q https://www.gutenberg.org/files/66057/66057-0.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 64 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 61.8 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ5HeAePgWV_"
      },
      "source": [
        "#import appropriate libraries and create Spark and SQL session \n",
        "import os\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alME2U1hKkY9",
        "outputId": "335d3d2f-1632-469e-9c7a-526c852eb81b"
      },
      "source": [
        "# create removePunctuation function to clean data \n",
        "import re\n",
        "import string\n",
        "def removePunctuation(text):\n",
        "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
        "\n",
        "    Note:\n",
        "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
        "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
        "        punctuation is removed.\n",
        "\n",
        "    Args:\n",
        "        text (str): A string.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned up string.\n",
        "    \"\"\"\n",
        "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    return regex.sub('', text).lower().strip()\n",
        "    \n",
        "print (removePunctuation('YŪGAO'))\n",
        "print (removePunctuation(' No under_score!'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yūgao\n",
            "no underscore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btI-jEpvJOZt",
        "outputId": "620ab5c6-0046-4c27-ab2c-e5005fa6a36c"
      },
      "source": [
        "# create removePunctuation function that does not lower case items, removes numbers, and removes roman numerals \n",
        "import re\n",
        "import string\n",
        "def removePunctuationupper(text):\n",
        "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
        "\n",
        "    Note:\n",
        "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
        "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
        "        punctuation is removed.\n",
        "\n",
        "    Args:\n",
        "        text (str): A string.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned up string.\n",
        "    \"\"\"\n",
        "    replace_dashes = text.replace(\"-\",\" \")\n",
        "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    data = regex.sub('', replace_dashes).strip()\n",
        "    remove_numbers = re.compile('[0-9]')\n",
        "    data_no_numbers = remove_numbers.sub(' ',data)\n",
        "    remove_numerals = re.compile(r'(?:^|\\s)(IX|IV|V?I{0,3})(?:\\s|$)')\n",
        "    return remove_numerals.sub(' ',data_no_numbers)\n",
        "\n",
        "#Notice the difference between the \n",
        "print (removePunctuationupper('YŪGAO12'))\n",
        "print (removePunctuationupper('Lady-in-Perpetual-Attendance'))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YŪGAO \n",
            "Lady in Perpetual Attendance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFiIaXshKFfi"
      },
      "source": [
        "#create function to remove additional types of punctuation that may have been missed by the removePunctuation function \n",
        "import sys\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "punc_list=[]\n",
        "\n",
        "for i in range(0, sys.maxunicode + 1):\n",
        "  if unicodedata.category(chr(i)) in ('Pc', 'Pd', 'Pe', 'Pf', 'Pi', 'Po', 'Ps'):\n",
        "    punc_list.append(chr(i))\n",
        "\n",
        "punc_string=''.join(punc_list)\n",
        "\n",
        "def removeUniPunctuation(text):\n",
        "    regex = re.compile('[%s]' % re.escape(punc_string))\n",
        "    return regex.sub(' ', text).strip()"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugNbYuORgpYZ",
        "outputId": "f8379be0-5bc4-4daf-87cf-0b239e3c040a"
      },
      "source": [
        "#import files and parse to 10 different mappers \n",
        "from pyspark.sql.functions import regexp_extract, col\n",
        "\n",
        "fileName = sc.textFile('/content/sample_data/66057-0.txt',10)\n",
        "\n",
        "remove_numerals = re.compile(r'(?:^|\\s)(IX|IV|V?I{0,3})(?:\\s|$)')\n",
        "\n",
        "#first attempt to remove punctuation from data \n",
        "fileName2 = sc.textFile('/content/sample_data/66057-0.txt',10).map(lambda x: removeUniPunctuation(x))\n",
        "\n",
        "#split all words into their own location using .split \n",
        "all_words_split = fileName.flatMap(lambda x: x.split())\n",
        "\n",
        "\n",
        "#search for uppercase letters using regex, map roman number data cleaning logic to strings if the length of the value is > 1, split data into individual cells and count occurrences  \n",
        "total_uppercase = fileName2.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count()\n",
        "\n",
        "#search for lowercase letters using regex, map roman number data cleaning logic to strings if the length of the value is > 1, split data into individual cells and count occurrences  \n",
        "total_lowercase = fileName2.flatMap(lambda x: re.findall(r'\\b[a-z]+(?:\\s+[a-z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count()\n",
        "\n",
        "print('total number of uppercase words: ', total_uppercase)\n",
        "print('total number of lowercase words: ', total_lowercase)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of uppercase words:  1403\n",
            "total number of lowercase words:  89356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_U345VgAYJp",
        "outputId": "a17adbe6-68e4-4e10-f3f4-eb86952bc728"
      },
      "source": [
        "#total count of all words in filename - 1. filename2 has logic that incorrectly expands the total word count, so it cannot be used. \n",
        "total_word_count  = all_words_split.count()\n",
        "total_word_count"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101277"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2gEaprGg7Qt"
      },
      "source": [
        "#read data accross 10 mappers \n",
        "#join string into one single line of text \n",
        "#split texts based on CHAPTER # and Appendix 3 \n",
        "\n",
        "import re\n",
        "\n",
        "book = sc.textFile('/content/sample_data/66057-0.txt',10).collect()\n",
        "\n",
        "book_single_string = ' '.join(book)\n",
        "\n",
        "chapters = re.split('CHAPTER \\w{1,4} | APPENDIX \\w{1}',book_single_string)\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyXGpL_Y8zpR",
        "outputId": "c377258b-6ee9-4a1d-ad9f-1f97a4dfe564"
      },
      "source": [
        "#parralize chapter data into their own RDD for analysis \n",
        "intro = sc.parallelize([chapters[0]])\n",
        "chapter1 = sc.parallelize([chapters[1]])\n",
        "chapter2 = sc.parallelize([chapters[2]])\n",
        "chapter3 = sc.parallelize([chapters[3]])\n",
        "chapter4 = sc.parallelize([chapters[4]])\n",
        "chapter5 = sc.parallelize([chapters[5]])\n",
        "chapter6 = sc.parallelize([chapters[6]])\n",
        "chapter7 = sc.parallelize([chapters[7]])\n",
        "chapter8 = sc.parallelize([chapters[8]])\n",
        "chapter9 = sc.parallelize([chapters[9]])\n",
        "appendix = sc.parallelize([chapters[10] + chapters[11]])\n",
        "\n",
        "#run punctuation logic and place word values into their own lists location \n",
        "intro_rdd_word_count = intro.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter1_rdd_word_count = chapter1.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter2_rdd_word_count = chapter2.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter3_rdd_word_count = chapter3.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter4_rdd_word_count = chapter4.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter5_rdd_word_count = chapter5.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter6_rdd_word_count = chapter6.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter7_rdd_word_count = chapter7.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter8_rdd_word_count = chapter8.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "chapter9_rdd_word_count = chapter9.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "appendix_rdd_word_count = appendix.map(removeUniPunctuation).flatMap(lambda x: x.split())\n",
        "\n",
        "\n",
        "#percentage of uppercase values by introduction, chapter, appendix \n",
        "print('percentage of uppercase values in introduction: ', round(intro_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 1: ',round(chapter1_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 2: ',round(chapter2_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 3: ',round(chapter3_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 4: ',round(chapter4_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 5: ',round(chapter5_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 6: ',round(chapter6_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 7: ',round(chapter7_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 8: ',round(chapter8_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in chapter 9: ',round(chapter9_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "print('percentage of uppercase values in appendix: ',round(appendix_rdd_word_count.flatMap(lambda x: re.findall(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b',x)).map(lambda x: remove_numerals.sub('',x) if len(x) > 1 else x).flatMap(lambda x: x.split()).count() / total_uppercase * 100,2))\n",
        "\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "percentage of uppercase values in introduction:  6.27\n",
            "percentage of uppercase values in chapter 1:  4.35\n",
            "percentage of uppercase values in chapter 2:  18.96\n",
            "percentage of uppercase values in chapter 3:  2.71\n",
            "percentage of uppercase values in chapter 4:  13.11\n",
            "percentage of uppercase values in chapter 5:  16.18\n",
            "percentage of uppercase values in chapter 6:  7.7\n",
            "percentage of uppercase values in chapter 7:  4.21\n",
            "percentage of uppercase values in chapter 8:  2.14\n",
            "percentage of uppercase values in chapter 9:  8.55\n",
            "percentage of uppercase values in appendix:  16.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRpQVXr9ir5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5821a0c-903a-4dd1-cd27-497dd36d41ad"
      },
      "source": [
        "#import schema related functions \n",
        "from pyspark.sql.types import StructType\n",
        "from pyspark.sql.types import StructField\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import lower, col\n",
        "\n",
        "#look for these words in the dataframe \n",
        "word_search = ['Aoi','Emperor','Genji','Kiritsubo', 'Kōkiden','Koremitsu']\n",
        "\n",
        "#create datafrme of wordsearch values \n",
        "df_word_search = sqlContext.createDataFrame(word_search, StringType())\n",
        "\n",
        "#create data frame with remove punctuation - upper function \n",
        "dfintro = sqlContext.createDataFrame(intro_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df1 = sqlContext.createDataFrame(chapter1_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df2 = sqlContext.createDataFrame(chapter2_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df3 = sqlContext.createDataFrame(chapter3_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df4 = sqlContext.createDataFrame(chapter4_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df5 = sqlContext.createDataFrame(chapter5_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df6 = sqlContext.createDataFrame(chapter6_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df7 = sqlContext.createDataFrame(chapter7_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df8 = sqlContext.createDataFrame(chapter8_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "df9 = sqlContext.createDataFrame(chapter9_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "dfappendix = sqlContext.createDataFrame(appendix_rdd_word_count.map(removePunctuationupper), StringType())\n",
        "\n",
        "#count the occurends of the word search values by chapter \n",
        "print('introduction count: ',dfintro[dfintro['value'].isin(word_search)].count())\n",
        "print('chapter 1 count: ', df1[df1['value'].isin(word_search)].count())\n",
        "print('chapter 2 count: ',df2[df2['value'].isin(word_search)].count())\n",
        "print('chapter 3 count: ',df3[df3['value'].isin(word_search)].count())\n",
        "print('chapter 4 count: ',df4[df4['value'].isin(word_search)].count())\n",
        "print('chapter 5 count: ',df5[df5['value'].isin(word_search)].count())\n",
        "print('chapter 6 count: ',df6[df6['value'].isin(word_search)].count())\n",
        "print('chapter 7 count: ',df7[df7['value'].isin(word_search)].count())\n",
        "print('chapter 8 count: ',df8[df8['value'].isin(word_search)].count())\n",
        "print('chapter 9 count: ',df9[df9['value'].isin(word_search)].count())\n",
        "print('appendix count: ',dfappendix[dfappendix['value'].isin(word_search)].count())\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "introduction count:  46\n",
            "chapter 1 count:  83\n",
            "chapter 2 count:  93\n",
            "chapter 3 count:  30\n",
            "chapter 4 count:  155\n",
            "chapter 5 count:  156\n",
            "chapter 6 count:  85\n",
            "chapter 7 count:  131\n",
            "chapter 8 count:  50\n",
            "chapter 9 count:  189\n",
            "appendix count:  14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdLtWtI7iiF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a499ae1-ce16-4330-807b-797c32baedc4"
      },
      "source": [
        "#read in file name and run removePunctuation - upper to clean data \n",
        "filename_with_data_cleaned_when_create = fileName.map(removePunctuationupper)\n",
        "\n",
        "#split word values into seperate list locations \n",
        "file_name_prep_for_unique_count = filename_with_data_cleaned_when_create.flatMap(lambda x: x.split())\n",
        "\n",
        "#count words, reduce by key values and count them to obtain unique word values \n",
        "unique_count_of_words = file_name_prep_for_unique_count.map(lambda word: (word,1)).reduceByKey(lambda a,b: a+b).count()\n",
        "\n",
        "print('number of unique words: ', unique_count_of_words)\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of unique words:  9379\n"
          ]
        }
      ]
    }
  ]
}